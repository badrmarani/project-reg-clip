{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from skimage import io\n",
    "\n",
    "from src.models.resnet import ResNet\n",
    "from src.utils import update_state_dict, extract_keywords, is_keyword_in_caption\n",
    "from src.datasets.loader import get_loaders\n",
    "from src.datasets import CelebA\n",
    "from src.metrics import WorstGroupAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CelebA(\"../../datasets/celeba\", \"Male\", 2)\n",
    "test_loader = get_loaders(\n",
    "    test_dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['filename', 'caption', 'image', 'label', 'spurious_label', 'group'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 3\n",
    "num_groups = num_classes**2\n",
    "\n",
    "y_true = torch.randint(0, num_classes, size=(batch_size,))\n",
    "y_pred = torch.randint(0, num_classes, size=(batch_size,))\n",
    "g = batch[\"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:\t[0, 2, 1, 2, 1, 0, 0, 0, 1, 1]\n",
      "y_true:\t[2, 1, 1, 0, 1, 1, 1, 2, 2, 2]\n",
      "g:\t[0, 0, 0, 0, 0, 1, 1, 0, 0, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"y_pred:\\t{y_pred.tolist()}\\n\"\n",
    "    f\"y_true:\\t{y_true.tolist()}\\n\"\n",
    "    f\"g:\\t{g.tolist()}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def similarity(keywords, root_dir, image_filenames, device):    \n",
    "    text_embeddings = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in keywords])\n",
    "\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    images = [\n",
    "        Image.fromarray(io.imread(os.path.join(root_dir, f))) for f in image_filenames\n",
    "    ]\n",
    "    images = torch.cat([preprocess(im).unsqueeze(0) for im in images])\n",
    "\n",
    "    text_embeddings = model.encode_text(text_embeddings)\n",
    "    image_embeddings = model.encode_image(images)\n",
    "\n",
    "    image_embeddings /= image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    similarity = (\n",
    "        100.0 * image_embeddings @ text_embeddings.T\n",
    "    )  # size: (len(image_filenames), 20)\n",
    "    similarity = similarity.mean(dim=0)  # size: (20,)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# calculate the similarity between the keyword and the correct set, as well\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# as the similarity between the keyword and the incorrect set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m filenames_correct_set \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m j, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m t[\u001b[38;5;241m0\u001b[39m, j]]\n\u001b[0;32m---> 14\u001b[0m sim_k_correct_set \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_filenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilenames_correct_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../datasets/celeba/img_align_celeba/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# size: (num_keywords,)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m filenames_incorrect_set \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m j, f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m t[\u001b[38;5;241m1\u001b[39m, j]]\n\u001b[1;32m     22\u001b[0m sim_k_incorrect_set \u001b[38;5;241m=\u001b[39m similarity(\n\u001b[1;32m     23\u001b[0m     keywords\u001b[38;5;241m=\u001b[39mkeywords,\n\u001b[1;32m     24\u001b[0m     image_filenames\u001b[38;5;241m=\u001b[39mfilenames_incorrect_set,\n\u001b[1;32m     25\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../datasets/celeba/img_align_celeba/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     26\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/badr/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[126], line 9\u001b[0m, in \u001b[0;36msimilarity\u001b[0;34m(keywords, root_dir, image_filenames, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/32\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     Image\u001b[38;5;241m.\u001b[39mfromarray(io\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, f))) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m image_filenames\n\u001b[1;32m      8\u001b[0m ]\n\u001b[0;32m----> 9\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_text(text_embeddings)\n\u001b[1;32m     12\u001b[0m image_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(images)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "t = torch.empty(size=(2, batch_size), dtype=torch.bool)\n",
    "for i in range(num_classes):\n",
    "    # retrieve the indices where the model mispredicts the labels of the images\n",
    "    t[0, :] = ((y_pred == y_true) & (y_true == i)).bool()  # correct set\n",
    "    t[1, :] = ((y_pred != y_true) & (y_true == i)).bool()  # incorrect set\n",
    "\n",
    "    # extract keywords from the correct set\n",
    "    caption = \" \".join([c for j, c in enumerate(batch[\"caption\"]) if t[1, j]])\n",
    "    keywords = extract_keywords(caption)\n",
    "\n",
    "    # calculate the similarity between the keyword and the correct set, as well\n",
    "    # as the similarity between the keyword and the incorrect set\n",
    "    filenames_correct_set = [f for j, f in enumerate(batch[\"filename\"]) if t[0, j]]\n",
    "    sim_k_correct_set = similarity(\n",
    "        keywords=keywords,\n",
    "        image_filenames=filenames_correct_set,\n",
    "        root_dir=\"../../datasets/celeba/img_align_celeba/\",\n",
    "        device=device,\n",
    "    )  # size: (num_keywords,)\n",
    "\n",
    "    filenames_incorrect_set = [f for j, f in enumerate(batch[\"filename\"]) if t[1, j]]\n",
    "    sim_k_incorrect_set = similarity(\n",
    "        keywords=keywords,\n",
    "        image_filenames=filenames_incorrect_set,\n",
    "        root_dir=\"../../datasets/celeba/img_align_celeba/\",\n",
    "        device=device,\n",
    "    )  # size: (num_keywords,)\n",
    "\n",
    "    # clip_score = sim_k_correct_set - sim_k_incorrect_set  # size: (num_keywords,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = filenames_incorrect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = batch[\"image\"][t[1, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 77])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in keywords])\n",
    "text_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = images.device\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"../../datasets/celeba/img_align_celeba/\"\n",
    "\n",
    "images = [Image.fromarray(io.imread(os.path.join(root_dir, f))) for f in filenames]\n",
    "images = torch.cat([preprocess(im).unsqueeze(0) for im in images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.0033, 24.3015, 21.6432, 21.5929, 24.8060, 21.9088, 21.8825, 23.3264,\n",
       "        20.3384, 22.4032, 23.5213, 22.1934, 23.2563, 24.3473, 20.0536, 21.7447,\n",
       "        22.7959, 22.4792, 21.7573, 23.2692])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
